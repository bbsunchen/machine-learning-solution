\documentclass{amsart}
\usepackage{fullpage}
\usepackage[procnames]{listings}
\usepackage{color}
\newcommand{\set}[1]{\ensuremath{\left\{#1\right\}}}
\newtheorem{quest}{Question}
\usepackage{mathtools}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\begin{document}
\title{CSE584 Homework 5}
\author{Chen Sun\\cxs1031@psu.edu}
\maketitle


\section{Question 1}

\noindent(1)Observed variables: $x_1, ..., x_n$\\
(2)Missing random variables: \\

\begin{align*}
\text{mean cluster of point }x_j \text{, denote it as a vector } A_j &= \begin{cases}
	[1, 0] & \text{ if mean cluster is 1}\\
	[0, 1] & \text{ if mean cluster is 2}
\end{cases}\\
\text{variance cluster of point }x_j \text{, denote it as a vector } B_j &= \begin{cases}
[1, 0] & \text{ if variance cluster is 1}\\
[0, 1] & \text{ if variance cluster is 2}
\end{cases}\\
\end{align*}
(3)Unknown parameters: $\pi_1, \pi_2, \gamma_1, \gamma_2, \mu_1, \mu_2, \sigma^2_1, \sigma^2_2$.
\\\\
Complete data likelihood function:
\\
\begin{align*}
	p &= \prod\limits_{j=1}^{n} \prod_{i=1}^{2} \prod_{k=1}^{2} \left[ \dfrac{1}{\sigma_k\sqrt{2\pi}}e^{-\frac{(x_j-\mu_i)^2}{2\sigma^2_k}}\pi_i \gamma_k \right]^{A_j[i]\times B_j[k]}
\end{align*}
\\
Full data loglikelihood function:
\\
\begin{align*}
p &= \sum\limits_{j=1}^{n} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_j[i] B_j[k]} \left[ -\log\sigma_k -\frac{(x_j-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]
\end{align*}
\\
$\Lambda$ function:
\begin{align*}
	\Lambda &= \sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\sum\limits_{j=1}^{n} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_j[i] B_j[k]} \left[ -\log\sigma_k -\frac{(x_j-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&-\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\left[\log{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\right]
\end{align*}
\\
Simplifying steps:

\begin{align*}
	&\qquad\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\left[\log{q_1(A_1)s_1(B_1)}\right]\\
	&=\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\left[\log({q_1(A_1)s_1(B_1)})\right]\\
	&=\sum_{A_1B_1}{q_1(A_1)s_1(B_1)\log(q_1(A_1)s_1(B_1))}\left[\sum_{A_2 \dots A_n B_2 \dots B_n}{q_2(A_2)\dots q_n(A_n) s_2(B_2) \dots s_n(B_n)} \right]\\
	&=\sum_{A_1B_1}{q_1(A_1)s_1(B_1)\log(q_1(A_1)s_1(B_1))}
\end{align*}

So:
\begin{align*}
	&\quad\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\left[\log{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\right]\\
	&=\sum_{A_1B_1}q_1(A_1)s_1(B_1) + \dots + \sum_{A_nB_n}q_n(A_n)s_n(B_n)\\\\
	&\quad\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)}\sum\limits_{j=1}^{n} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_j[i] B_j[k]} \left[ -\log\sigma_k -\frac{(x_j-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&=\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&\quad+\sum_{A_1 \dots A_n B_1 \dots B_n}{q_1(A_1)\dots q_n(A_n) s_1(B_1) \dots s_n(B_n)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_2[i] B_2[k]} \left[ -\log\sigma_k -\frac{(x_2-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&\quad + \dots\\
	&=\sum_{A_1 B_1}{q_1(A_1)s_1(B_1)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&\quad+\sum_{A_2 B_2}{q_2(A_2) s_2(B_2)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_2[i] B_2[k]} \left[ -\log\sigma_k -\frac{(x_2-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&\quad + \dots\\
\end{align*}

So the original $\Lambda$ function can be simplified as:
\begin{align*}
	&\sum_{A_1 B_1}{q_1(A_1)s_1(B_1)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] - \sum_{A_1 B_1}{q_1(A_1)s_1(B_1)}\log({q_1(A_1)s_1(B_1)}) \\
	&+\dots\\
	&+\sum_{A_n B_n}{q_n(A_n)s_n(B_n)} \sum_{i=1}^{2} \sum_{k=1}^{2} {A_n[i] B_n[k]} \left[ -\log\sigma_k -\frac{(x_n-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] - \sum_{A_n B_n}{q_n(A_n)s_n(B_n)}\log({q_n(A_n)s_n(B_n)})
\end{align*}
\\
There are $2n$ formulations in $\Lambda$ function, considering the first two formulation:
\\$\sum\limits_{A_1 B_1}{q_1(A_1)s_1(B_1)} \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] - \sum_{A_1 B_1}{q_1(A_1)s_1(B_1)}\log({q_1(A_1)s_1(B_1)})$\\\\

Denote $h(A_1B_1) = \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]$ \\\\
By KL-divergence trick, $q_1(A_1)s_1(B_1) = \dfrac{e^{h(A_1B_1)}}{\sum\limits_{A_1B_1} {e^{h(A_1 B_1)}} }$\\\\

So the probability under $q\cdot s$ that first point has mean from cluster $i$ and deviation from cluster $k$ is:
$T_1[i,k] = \dfrac{\dfrac{\pi_i\gamma_k}{\sigma_k}e^{-\frac{(x_1-\mu_i)^2}{2\sigma^2_k}}}{\sum\limits_{\iota=1}^{2}\sum\limits_{\kappa=1}^{2}\dfrac{\pi_\iota\gamma_\kappa}{\sigma_\kappa}e^{-\frac{(x_1-\mu_\iota)^2}{2\sigma^2_\kappa}}}$\\

For the first formulation of $\Lambda$ function, plugging in:
\begin{align*}
	&\quad\sum\limits_{A_1 B_1}{q_1(A_1)s_1(B_1)} \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2} {A_1[i] B_1[k]} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\\
	&=\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2} \left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]\sum\limits_{A_1 B_1}{q_1(A_1)s_1(B_1)}{A_1[i] B_1[k]} \\
	&=\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_1[i,k]\left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right]
\end{align*}

\subsection*{$q$ update}

So after $q$ update, the $\Lambda$ function is:
\begin{align*}
	\Lambda&= \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_1[i,k]\left[ -\log\sigma_k -\frac{(x_1-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] + \text{something }\\
	&\quad+ \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_2[i,k]\left[ -\log\sigma_k -\frac{(x_2-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] + \text{something }\\
	&\quad+ \dots\\
	&=\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_j[i,k]\left[ -\log\sigma_k -\frac{(x_j-\mu_i)^2}{2\sigma^2_k} + \log\pi_i + \log\gamma_k \right] + \text{something}
	\text{, where }T_j[i,k] = \dfrac{\dfrac{\pi_i\gamma_k}{\sigma_k}e^{-\frac{(x_j-\mu_i)^2}{2\sigma^2_k}}}{\sum\limits_{\iota=1}^{2}\sum\limits_{\kappa=1}^{2}\dfrac{\pi_\iota\gamma_\kappa}{\sigma_\kappa}e^{-\frac{(x_j-\mu_\iota)^2}{2\sigma^2_\kappa}}}
\end{align*}

\subsection*{$\mu_i$ update} Considering $\mu_i$:
\begin{align*}
	\mu_i &= argmax \sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}T_j[i,k]\left[-\frac{(x_j-\mu_i)^2}{2\sigma^2_k}\right]\\
	\dfrac{\partial}{\partial \mu_i} &= \sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}\frac{T_j[i,k]}{\sigma^2_k}\left(x_j-\mu_i \right)\\
	\mu_i&=\dfrac{\sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}\frac{T_j[i,k]}{\sigma^2_k} x_j}{\sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}\frac{T_j[i,k]}{\sigma^2_k}}
\end{align*}

\subsection*{\bf{$\pi$} update}

\begin{align*}
\pi &= argmax \sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_j[i,k]\log\pi_i\text{, such that } \sum\limits_{i=1}^{2}\pi_i = 1
\end{align*}\\
Introducing lagrange multiplier $\lambda$:\\ 
\begin{align*}
\pi &= argmax \sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_j[i,k]\log\pi_i + \lambda(\sum\limits_{i=1}^{2}\pi_i - 1)
\end{align*}\\

\begin{align*}
	\dfrac{\partial}{\partial \pi_1} &= \sum\limits_{j=1}^{n} \sum\limits_{k=1}^{2}\dfrac{T_j[1,k]}{\pi_1} + \lambda = 0\\
	\text{so: }&{\pi_1} \lambda + \sum\limits_{j=1}^{n} \sum\limits_{k=1}^{2}T_j[1,k] = 0\\\\
	\dfrac{\partial}{\partial \pi_2} &= \sum\limits_{j=1}^{n} \sum\limits_{k=1}^{2}\dfrac{T_j[2,k]}{\pi_2} + \lambda = 0\\
	\text{so: }&{\pi_2} \lambda + \sum\limits_{j=1}^{n} \sum\limits_{k=1}^{2}T_j[2,k] = 0
\end{align*}

Add them up:

\begin{align*}
\lambda + \sum\limits_{j=1}^{n} \sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_j[i,k] = 0
\end{align*}

Since we have $\sum\limits_{i=1}^{2} \sum\limits_{k=1}^{2}T_j[i,k] = 1$, so $\lambda + n = 0$.

So $\lambda = -n$.

So $\pi_1 = \dfrac{\sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}T_j[1,k]}{n}$; $\pi_2 = \dfrac{\sum\limits_{j=1}^{n}\sum\limits_{k=1}^{2}T_j[2,k]}{n}$.

\subsection*{$\sigma^2_k$ update}

\begin{align*}
\text{let }t_k &= \sigma^2_k\\
t_k &= argmax \sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2} T_j[i,k]\left[ -\log\sqrt{t_k} -\frac{(x_j-\mu_i)^2}{2t_k}\right]\\
\dfrac{\partial}{\partial t_k} &= \sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2} T_j[i,k]\left[ -\frac{1}{2t_k} +\frac{(x_j-\mu_i)^2}{2t_k^2}\right] = 0\\
\sigma_k^2 &= t_k = \dfrac{\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2}T_j[i,k](x_j-\mu_i)^2}{\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2}T_j[i,k]}
\end{align*}

\subsection*{$\gamma$ update} Similar as $\pi$ update, we have:
So $\gamma_1 = \dfrac{\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2}T_j[i,1]}{n}$; $\gamma_2 = \dfrac{\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{2}T_j[i,2]}{n}$.

\end{document}